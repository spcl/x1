# Copyright (c) 2025 ETH Zurich.
#                    All rights reserved.
#
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.
#
# main author: Eric Schreiber
#
# contributions: Afonso Catarino

import argparse
import os
import threading
import time

import numpy as np
from flask import current_app

from x1.server import get_consecutive_sample_count_with_max_constraint
from x1.server.policy_server import init_app

parser = argparse.ArgumentParser(description="Training Server")
parser.add_argument(
    "--ipnport", type=int, default=12434, help="Port to run the server on"
)
parser.add_argument("--gpu", type=str, default="cuda", help="GPU to run the server on")
args = parser.parse_args()

app = init_app(args.gpu)


def batched_compute(data_list: list[dict]) -> list[dict]:
    """
    Process a batch of requests and return results.

    Args:
        data_list(list[dict]): List of samples.
    Returns:
        list[dict]: List of prompts and model outputs.
    """
    input_questions = [data.get("question", None) for data in data_list]
    input_contexts = [data.get("context", None) for data in data_list]
    safe_logits = np.any(
        np.array([data.get("safe_logits", False) for data in data_list])
    )
    num_samples = int(
        np.max(np.array([data.get("num_samples", 1) for data in data_list]))
    )
    temperatures = np.array([data.get("temperature", 0.7) for data in data_list])
    max_input_length = np.max(
        np.array([data.get("max_input_length", 1024) for data in data_list])
    )
    max_output_length = np.max(
        np.array([data.get("max_output_length", 1152) for data in data_list])
    )
    decoding_strategies = np.array(
        [data.get("decoding_strategy", "temperature") for data in data_list]
    )
    simulate_to_ends = np.array(
        [data.get("full_simulation", False) for data in data_list]
    )

    # Make sure all the temperatures, decoding_strategies, and simulate_to_ends are the same
    assert np.all(temperatures == temperatures[0])
    assert np.all(decoding_strategies == decoding_strategies[0])
    assert np.all(simulate_to_ends == simulate_to_ends[0])

    if simulate_to_ends[0]:
        (
            results,
            logits,
            prompts,
            stop_strings_founds,
            is_final_steps,
        ) = current_app.config["MODEL_MANAGER"].simulate_to_end(
            input_questions=input_questions,
            input_contexts=input_contexts,
            max_input_length=max_input_length,
            max_output_length=max_output_length,
            num_samples=num_samples,
            output_logits=safe_logits,
            temperature=temperatures[0],
            decoding_strategy=decoding_strategies[0],
        )
    else:
        (
            results,
            logits,
            prompts,
            stop_strings_founds,
            is_final_steps,
        ) = current_app.config["MODEL_MANAGER"].get_reasoning_steps(
            input_questions=input_questions,
            input_contexts=input_contexts,
            max_input_length=max_input_length,
            max_output_length=max_output_length,
            num_samples=num_samples,
            output_logits=safe_logits,
            temperature=temperatures[0],
            decoding_strategy=decoding_strategies[0],
        )

    if safe_logits:
        logit_files = os.listdir(current_app.config["LOGITS_PATH"])

        logit_files = [f for f in logit_files if f.startswith("logits")]
        if len(logit_files) == 0:
            logit_file = "logits_0.json"
        else:
            logit_file = f'logits_{max([int(f.split("_")[-1].split(".")[0]) for f in logit_files]) + 1}.json'
        # Store logits using numpy
        np.save(
            f'{os.path.join(current_app.config["LOGITS_PATH"], logit_file)}',
            logits,
            allow_pickle=True,
        )
        # Append the sentence generated by the model to the result and the path to the logits file
        # in a csv format.
        with open(f'{current_app.config["FORWARD_PASS_FILE_PATH"]}', "a") as f:
            f.write(f"{logit_file},{results}\n")
        with open(f'{current_app.config["PROMPT_FILE_PATH"]}', "a") as f:
            f.write(f"{logit_file},\n{prompts}\n")

    return_dicts = []
    for i in range(len(input_questions)):
        return_dicts.append(
            {
                "result": results[i * num_samples : (i + 1) * num_samples],
                "prompt": prompts[i],
                "stop_string_found": stop_strings_founds[
                    i * num_samples : (i + 1) * num_samples
                ],
                "is_final_step": is_final_steps[
                    i * num_samples : (i + 1) * num_samples
                ],
            }
        )
    return return_dicts


def process_queue(queue: list) -> bool:
    """
    Process the queue in batches.

    Args:
        queue (list): List of samples to process.
    Returns:
        bool: True if the queue was processed, False otherwise.
    """

    if len(queue) == 0:
        return False

    # Estimate the maximum batch size. If you want to pack optimally you need to change how to
    # choose the number of samples in the generate function.
    batch_size = get_consecutive_sample_count_with_max_constraint(
        queue, current_app.config["INFERENCE_BATCH_SIZE"]
    )
    print(
        f"Processing queue of length {len(queue)} at {time.time()} with batch size {batch_size}.",
        flush=True,
    )
    if len(queue) >= batch_size:
        batch = [queue.popleft() for _ in range(batch_size)]
    elif len(queue) > 0 and time.time() - queue[0][1] >= 2:
        batch = [queue.popleft() for _ in range(min(batch_size, len(queue)))]
    else:
        return False

    print(f"Processing ids: {[batch[i][2] for i in range(len(batch))]}", flush=True)

    result = batched_compute([data[0] for data in batch])
    print(
        f"Finished processing ids: {[batch[i][2] for i in range(len(batch))]} at {time.time()}",
        flush=True,
    )
    for i in range(len(batch)):
        current_app.config["INFERENCE_FINISHED_DICT"][batch[i][2]] = result[i]
    return True


def process_batch() -> None:
    """
    Process batches asynchronously.
    If there are 8 or more requests or 5 seconds have passed, run batched_compute.
    """
    with app.app_context():
        while True:
            # Don't run inference when in training mode
            if current_app.config["ALLOW_TRAINING"]:
                current_app.config["ALLOW_TRAINING"] = False
                computed_queue_this_round = process_queue(
                    current_app.config["INFERENCE_STEP_QUEUE"]
                )
                computed_queue_this_round |= process_queue(
                    current_app.config["INFERENCE_SIMULATION_QUEUE"]
                )
                current_app.config["ALLOW_TRAINING"] = True
                if not computed_queue_this_round:
                    time.sleep(2)
            else:
                time.sleep(2)


if __name__ == "__main__":
    with app.app_context():
        current_app.config["TRAINING_THREAD"] = threading.Thread(
            target=process_batch, daemon=True
        )
        current_app.config["TRAINING_THREAD"].start()

    print(
        f"Starting server on port {args.ipnport} and putting model on {args.gpu}.",
        flush=True,
    )
    app.run(host="0.0.0.0", port=args.ipnport, debug=False)
